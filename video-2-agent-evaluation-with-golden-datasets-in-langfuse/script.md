# Script for Video 2: Agent Evaluation with Golden Datasets in Langfuse

## Introduction (0:00–0:45)
**Narration:**
Golden datasets serve as reliable benchmarks for evaluating agent performance. In this video, we'll run an agent evaluation in Langfuse using a predefined golden dataset and learn how to interpret the results.

## Uploading/Configuring Golden Dataset (0:45–2:00)
**Narration:**
First, navigate to the Langfuse evaluation interface and upload or select the provided golden dataset file (CSV or JSON). Ensure the dataset schema aligns with your agent's input and output format.

## Running the Evaluation (2:00–3:30)
**Narration:**
Launch the evaluation run by selecting your instrumented agent workflow and the golden dataset. Click "Start Evaluation" and wait for the run to complete.

## Reviewing Results (3:30–4:45)
**Narration:**
Once complete, review key metrics such as accuracy, precision, and recall. Click on failure cases to access associated trace logs for deeper analysis.

## Interpretation and Action (4:45–6:00)
**Narration:**
Use evaluation feedback to identify areas for improvement. Prioritize high-impact failures and iteratively refine your agent workflow for better performance.
